{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영상불러와서 OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def detect_objects_on_video(video_path, model, threshold=0.5):\n",
    "    # 비디오를 열고 비디오의 프레임을 처리합니다.\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # OpenCV는 BGR을 사용하지만, PyTorch는 RGB를 사용하므로 색상을 변환합니다.\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # 이미지를 PIL 이미지로 변환합니다.\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            # 모델을 호출하기 전에 이미지를 텐서로 변환합니다.\n",
    "            tensor_image = F.to_tensor(pil_image).unsqueeze(0)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                tensor_image = tensor_image.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 모델 예측\n",
    "                prediction = model(tensor_image)\n",
    "\n",
    "            # 이제 프레임에 객체 검출을 시각화할 수 있습니다.\n",
    "            # 예를 들어, bounding box와 함께 객체를 그립니다.\n",
    "            for i in range(prediction[0]['boxes'].shape[0]):\n",
    "                if prediction[0]['scores'][i] > threshold:\n",
    "                    box = prediction[0]['boxes'][i].cpu().numpy().astype(int)\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "            # 결과를 보여줍니다.\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            # 'q' 키를 누르면 종료합니다.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 모델 로드\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 객체 검출 실행\n",
    "detect_objects_on_video('path_to_your_video.mp4', model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내장카메라 with cv2 & resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO DATASET\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사람만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\nic\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\miniconda3\\envs\\nic\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def detect_objects_on_webcam(model, threshold=0.5):\n",
    "    # 웹캠으로부터 비디오를 캡처합니다.\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # OpenCV는 BGR을 사용하지만, PyTorch는 RGB를 사용하므로 색상을 변환합니다.\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # 이미지를 PIL 이미지로 변환합니다.\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            # 모델을 호출하기 전에 이미지를 텐서로 변환합니다.\n",
    "            tensor_image = F.to_tensor(pil_image).unsqueeze(0)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                tensor_image = tensor_image.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 모델 예측\n",
    "                prediction = model(tensor_image)\n",
    "\n",
    "            # 이제 프레임에 객체 검출을 시각화할 수 있습니다.\n",
    "            # 예를 들어, bounding box와 함께 객체를 그립니다.\n",
    "            for i in range(prediction[0]['boxes'].shape[0]):\n",
    "                # 'person' 클래스의 인덱스는 1입니다.\n",
    "                if prediction[0]['scores'][i] > threshold and prediction[0]['labels'][i] == 1:\n",
    "                    box = prediction[0]['boxes'][i].cpu().numpy().astype(int)\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "            # 결과를 보여줍니다.\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            # 'q' 키를 누르면 종료합니다.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 모델 로드\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 웹캠에서 객체 검출을 실행\n",
    "detect_objects_on_webcam(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def detect_objects_on_webcam(model, threshold=0.5):\n",
    "    # 웹캠으로부터 비디오를 캡처합니다.\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # OpenCV는 BGR을 사용하지만, PyTorch는 RGB를 사용하므로 색상을 변환합니다.\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # 이미지를 PIL 이미지로 변환합니다.\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            # 모델을 호출하기 전에 이미지를 텐서로 변환합니다.\n",
    "            tensor_image = F.to_tensor(pil_image).unsqueeze(0)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                tensor_image = tensor_image.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 모델 예측\n",
    "                prediction = model(tensor_image)\n",
    "\n",
    "            # 이제 프레임에 객체 검출을 시각화할 수 있습니다.\n",
    "            # 예를 들어, bounding box와 함께 객체를 그립니다.\n",
    "            for i in range(prediction[0]['boxes'].shape[0]):\n",
    "                if prediction[0]['scores'][i] > threshold:\n",
    "                    box = prediction[0]['boxes'][i].cpu().numpy().astype(int)\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "\n",
    "            # 결과를 보여줍니다.\n",
    "            cv2.imshow('frame', frame)\n",
    "\n",
    "            # 'q' 키를 누르면 종료합니다.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 모델 로드\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 웹캠에서 객체 검출을 실행\n",
    "detect_objects_on_webcam(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch with YOLO\n",
    "pip install -U torch torchvision cython\n",
    "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "git clone https://github.com/ultralytics/yolov5  # YOLOv5 라이브러리 복사\n",
    "cd yolov5\n",
    "pip install -r requirements.txt  # 종속성 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torch torchvision cython\n",
    "# !pip install -U git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "# %cd C:\\Users\\user\\Desktop\\project\\model\\yolov5\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install --upgrade yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\user\\\\Desktop\\\\project', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\python39.zip', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\DLLs', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\lib', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic', '', 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\lib\\\\site-packages', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\nic\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\project\\model\\yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\user/.cache\\torch\\hub\\master.zip\n",
      "WARNING  'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING  'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
      "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
      "    import torch\n",
      "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
      "    torch.save(ckpt, \"updated-model.pt\")\n",
      "\n",
      "YOLOv5  2023-7-17 Python-3.9.17 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True, force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in c:\\users\\user\\miniconda3\\envs\\nic\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall utils\n",
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-7-17 Python-3.9.17 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "attempt_load() got an unexpected keyword argument 'map_location'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m         cv2\u001b[39m.\u001b[39mrectangle(img, c1, c2, color, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# filled\u001b[39;00m\n\u001b[0;32m     71\u001b[0m         cv2\u001b[39m.\u001b[39mputText(img, label, (c1[\u001b[39m0\u001b[39m], c1[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m), \u001b[39m0\u001b[39m, tl \u001b[39m/\u001b[39m \u001b[39m3\u001b[39m, [\u001b[39m225\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m], thickness\u001b[39m=\u001b[39mtf, lineType\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mLINE_AA)\n\u001b[1;32m---> 73\u001b[0m detect()\n",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(save_img)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Initialize\u001b[39;00m\n\u001b[0;32m     16\u001b[0m device \u001b[39m=\u001b[39m select_device(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# GPU가 있다면 사용합니다.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m model \u001b[39m=\u001b[39m attempt_load(weights, map_location\u001b[39m=\u001b[39;49mdevice)  \u001b[39m# 모델을 로드합니다.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m stride \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(model\u001b[39m.\u001b[39mstride\u001b[39m.\u001b[39mmax())  \u001b[39m# model stride\u001b[39;00m\n\u001b[0;32m     19\u001b[0m imgsz \u001b[39m=\u001b[39m check_img_size(imgsz, s\u001b[39m=\u001b[39mstride)  \u001b[39m# img_size 검사\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: attempt_load() got an unexpected keyword argument 'map_location'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "\n",
    "from yolov5.models.experimental import attempt_load\n",
    "from yolov5.utils.dataloaders import LoadStreams, LoadImages\n",
    "from yolov5.utils.general import check_img_size, non_max_suppression#, scale_coords\n",
    "from yolov5.utils.torch_utils import select_device #, load_classifier, time_synchronized\n",
    "\n",
    "def detect(save_img=False):\n",
    "    source, weights, view_img, imgsz = '0', 'yolov5s.pt', True, 640  # 웹캠을 소스로 사용하려면 '0'을 사용합니다.\n",
    "\n",
    "    # Initialize\n",
    "    device = select_device('')  # GPU가 있다면 사용합니다.\n",
    "    model = attempt_load(weights, map_location=device)  # 모델을 로드합니다.\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # img_size 검사\n",
    "\n",
    "\n",
    "    # Webcam에서 영상을 가져옵니다.\n",
    "    dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names  # 클래스 이름 가져오기\n",
    "    colors = [[np.random.randint(0, 255) for _ in range(3)] for _ in names]  # 클래스별 색상 설정\n",
    "\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # FP32 모델을 실행합니다.\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.float()  # uint8 to fp32\n",
    "        img /= 255.0  # (0 - 255) to (0.0 - 1.0)\n",
    "\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        pred = model(img, augment=False)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, 0.4, 0.5)\n",
    "\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            im0 = im0s[i].copy()\n",
    "\n",
    "            # 테스트 결과를 보여줍니다.\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "            cv2.imshow(str(source), im0)\n",
    "            if cv2.waitKey(1) == ord('q'):  # 'q'를 누르면 종료\n",
    "                break\n",
    "\n",
    "    print('Done.')\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 : 실패\n",
    " - 원인 : yolov5.utils.datasets 설치 오류로 인해 실패\n",
    " - 시간이 짧아 원인분석 후 문제해결하기 어려우므로 다른 모델을 사용하도록 하겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yolov4\n",
    "yolov4.weights: https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
    "yolov4.cfg: https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg\n",
    "coco.names: https://github.com/AlexeyAB/darknet/blob/master/data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# YOLOv4에서 사용하는 클래스 이름을 불러옵니다.\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# YOLOv4 네트워크를 로드합니다.\n",
    "net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "\n",
    "# 웹캠 비디오 스트림을 시작합니다.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    layerOutputs = net.forward(output_layers_names)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0]*width)\n",
    "                center_y = int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "\n",
    "                x = int(center_x - w/2)\n",
    "                y = int(center_y - h/2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append((float(confidence)))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(img, label, (x, y+30), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Image', img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴인식 with mediapipe / OPERATING OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection() as face_detection:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    while not success:\n",
    "      success, image = cap.read()\n",
    "    \n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if results.detections:\n",
    "      for detection in results.detections:\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "        \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "    \n",
    "    # Exit on esc\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# YOLO 네트워크 불러오기\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# 웹캠에서 스트림을 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # 웹캠으로부터 이미지를 읽어오기\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # YOLO에서 사용하는 형태로 이미지 변환\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                # 객체 검출\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # 위치 조정\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # 노이즈 제거\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0,255,0), 2)\n",
    "            cv2.putText(img, label, (x, y + 30), font, 2, (0,255,0), 3)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
